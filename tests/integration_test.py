#!/usr/bin/env python3
"""
é›†æˆæµ‹è¯• - å®Œæ•´çš„ä½œæ–‡è¯„ä»·æµç¨‹

æµ‹è¯•åŸºäºOxyGentå’Œlangextractçš„å®Œæ•´ä½œæ–‡è¯„ä»·ç³»ç»Ÿ
"""

import asyncio
import json
import os
import time
from pathlib import Path
from typing import Dict, Any

import pytest
from dotenv import load_dotenv
from loguru import logger

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class EssayEvaluationIntegrationTest:
    """ä½œæ–‡è¯„ä»·ç³»ç»Ÿé›†æˆæµ‹è¯•"""
    
    def __init__(self):
        self.test_essays = self._load_test_essays()
        self.test_results = []
        
    def _load_test_essays(self) -> list:
        """åŠ è½½æµ‹è¯•ç”¨çš„ä½œæ–‡æ•°æ®"""
        return [
            {
                "title": "æˆ‘çš„å¦ˆå¦ˆ",
                "content": """æˆ‘çš„å¦ˆå¦ˆæ˜¯ä¸€ä½æ¸©æŸ”çš„è€å¸ˆã€‚å¥¹æœ‰ä¸€åŒæ˜äº®çš„çœ¼ç›ï¼Œå°±åƒå¤©ç©ºä¸­é—ªçƒçš„æ˜Ÿæ˜Ÿã€‚æ¯å¤©æ—©ä¸Šï¼Œå¦ˆå¦ˆéƒ½ä¼šä¸ºæˆ‘å‡†å¤‡ç¾å‘³çš„æ—©é¤ï¼Œè¿˜ä¼šæ¸©æŸ”åœ°å«æˆ‘èµ·åºŠã€‚

å¦ˆå¦ˆå·¥ä½œå¾ˆå¿™ï¼Œä½†å¥¹æ€»æ˜¯æŠ½æ—¶é—´é™ªæˆ‘åšä½œä¸šã€‚å½“æˆ‘é‡åˆ°ä¸ä¼šçš„é¢˜ç›®æ—¶ï¼Œå¦ˆå¦ˆä¼šè€å¿ƒåœ°æ•™æˆ‘ï¼Œä»ä¸å‘è„¾æ°”ã€‚å¥¹çš„ç¬‘å®¹åƒæ˜¥å¤©çš„èŠ±æœµä¸€æ ·ç¾ä¸½ï¼Œæ€»æ˜¯è®©æˆ‘æ„Ÿåˆ°æ¸©æš–ã€‚

æˆ‘çˆ±æˆ‘çš„å¦ˆå¦ˆï¼Œå¥¹æ˜¯ä¸–ç•Œä¸Šæœ€å¥½çš„å¦ˆå¦ˆã€‚æˆ‘è¦å¥½å¥½å­¦ä¹ ï¼Œé•¿å¤§ååƒå¦ˆå¦ˆä¸€æ ·å¸®åŠ©åˆ«äººã€‚""",
                "type": "narrative",
                "grade": "grade_3",
                "expected_highlights": ["ä¿®è¾æ‰‹æ³•", "æƒ…æ„Ÿè¡¨è¾¾", "ç»“æ„å®Œæ•´"]
            },
            {
                "title": "ç¾ä¸½çš„æ˜¥å¤©",
                "content": """æ˜¥å¤©æ¥äº†ï¼Œå¤§åœ°ç©¿ä¸Šäº†ç»¿è‰²çš„æ–°è¡£ã€‚å°è‰ä»æ³¥åœŸé‡Œæ¢å‡ºå¤´æ¥ï¼Œå¥½å¥‡åœ°çœ‹ç€è¿™ä¸ªä¸–ç•Œã€‚æŸ³æ ‘å§‘å¨˜æ¢³ç†ç€è‡ªå·±é•¿é•¿çš„ç»¿å‘ï¼Œåœ¨å¾®é£ä¸­è½»è½»æ‘†åŠ¨ã€‚

èŠ±å›­é‡Œï¼Œæ¡ƒèŠ±ç²‰çº¢ç²‰çº¢çš„ï¼Œåƒå°å§‘å¨˜çº¢æ¶¦çš„è„¸é¢Šã€‚è´è¶åœ¨èŠ±ä¸›ä¸­ç¿©ç¿©èµ·èˆï¼Œèœœèœ‚å¿™ç€é‡‡èŠ±èœœã€‚å°é¸Ÿåœ¨æå¤´æ­Œå”±ï¼Œå¥½åƒåœ¨è¯´ï¼š"æ˜¥å¤©çœŸç¾å•Šï¼"

æˆ‘å–œæ¬¢æ˜¥å¤©ï¼Œå› ä¸ºæ˜¥å¤©ç»™æˆ‘ä»¬å¸¦æ¥äº†å¸Œæœ›å’Œå¿«ä¹ã€‚""",
                "type": "descriptive", 
                "grade": "grade_4",
                "expected_highlights": ["æ‹Ÿäººæ‰‹æ³•", "æ¯”å–»", "æ„Ÿå®˜æå†™"]
            },
            {
                "title": "ä¸€æ¬¡éš¾å¿˜çš„ç»å†",
                "content": """ä¸Šä¸ªæ˜ŸæœŸå…­ï¼Œæˆ‘å’Œçˆ¸çˆ¸å»çˆ¬å±±ã€‚åˆšå¼€å§‹æˆ‘å¾ˆå…´å¥‹ï¼Œè¹¦è¹¦è·³è·³åœ°èµ°åœ¨å‰é¢ã€‚å¯æ˜¯çˆ¬åˆ°åŠå±±è…°ï¼Œæˆ‘å°±ç´¯å¾—æ°”å–˜ååï¼Œè…¿åƒçŒäº†é“…ä¸€æ ·æ²‰é‡ã€‚

æˆ‘æƒ³æ”¾å¼ƒï¼Œååœ¨çŸ³å¤´ä¸Šä¸æƒ³èµ°äº†ã€‚çˆ¸çˆ¸é¼“åŠ±æˆ‘è¯´ï¼š"åšæŒå°±æ˜¯èƒœåˆ©ï¼Œå±±é¡¶çš„é£æ™¯å¾ˆç¾ï¼" å¬äº†çˆ¸çˆ¸çš„è¯ï¼Œæˆ‘åˆæœ‰äº†åŠ›æ°”ã€‚

ç»ˆäºåˆ°äº†å±±é¡¶ï¼æˆ‘çœ‹åˆ°äº†ç¾ä¸½çš„é£æ™¯ï¼Œå¿ƒé‡Œç‰¹åˆ«è‡ªè±ªã€‚é€šè¿‡è¿™æ¬¡çˆ¬å±±ï¼Œæˆ‘æ˜ç™½äº†ä¸€ä¸ªé“ç†ï¼šåšä»»ä½•äº‹æƒ…éƒ½è¦åšæŒï¼Œä¸èƒ½åŠé€”è€ŒåºŸã€‚""",
                "type": "narrative",
                "grade": "grade_5", 
                "expected_highlights": ["å¿ƒç†æå†™", "å¯¹è¯", "å“²ç†æ„Ÿæ‚Ÿ"]
            }
        ]
    
    async def test_oxygent_mas_system(self):
        """æµ‹è¯•OxyGent MASç³»ç»ŸåŸºç¡€åŠŸèƒ½"""
        logger.info("ğŸ§ª æµ‹è¯•OxyGent MASç³»ç»Ÿ...")
        
        try:
            from oxygent import MAS, oxy, Config
            from oxygent.utils.env_utils import get_env_var
            
            # æ£€æŸ¥ç¯å¢ƒå˜é‡
            api_key = get_env_var("DEFAULT_LLM_API_KEY")
            if not api_key:
                logger.warning("âš ï¸ æœªé…ç½®APIå¯†é’¥ï¼Œè·³è¿‡LLMæµ‹è¯•")
                return True
            
            # é…ç½®ç®€å•çš„MASç³»ç»Ÿ
            Config.set_agent_llm_model("test_llm")
            
            oxy_space = [
                oxy.HttpLLM(  # type: ignore
                    name="test_llm",
                    api_key=api_key,
                    base_url=get_env_var("DEFAULT_LLM_BASE_URL"),
                    model_name=get_env_var("DEFAULT_LLM_MODEL_NAME"),
                    timeout=30,
                ),
                oxy.ChatAgent(  # type: ignore
                    name="test_agent",
                    desc="æµ‹è¯•æ™ºèƒ½ä½“",
                    prompt="ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•æ™ºèƒ½ä½“ï¼Œè¯·ç®€å•å›åº”ç”¨æˆ·çš„é—®å€™ã€‚"
                )
            ]
            
            # æµ‹è¯•MASåˆå§‹åŒ–å’ŒåŸºæœ¬è°ƒç”¨
            async with MAS(oxy_space=oxy_space) as mas:
                logger.info("âœ… MASç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
                
                # æµ‹è¯•ç®€å•è°ƒç”¨
                result = await mas.call(
                    callee="test_agent",
                    arguments={"query": "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•"}
                )
                
                logger.info(f"âœ… æ™ºèƒ½ä½“è°ƒç”¨æˆåŠŸ: {str(result)[:100]}...")
                
            return True
            
        except Exception as e:
            logger.error(f"âŒ OxyGent MASæµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def test_langextract_functionality(self):
        """æµ‹è¯•langextractåŸºç¡€åŠŸèƒ½"""
        logger.info("ğŸ§ª æµ‹è¯•langextractåŠŸèƒ½...")
        
        try:
            import langextract as lx
            
            # æµ‹è¯•åŸºæœ¬æ•°æ®ç»“æ„
            example = lx.data.ExampleData(
                text="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­ã€‚",
                extractions=[
                    lx.data.Extraction(
                        extraction_class="æµ‹è¯•ç±»åˆ«",
                        extraction_text="æµ‹è¯•å¥å­",
                        attributes={"type": "example"}
                    )
                ]
            )
            
            logger.info("âœ… langextractæ•°æ®ç»“æ„æµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•æå–åŠŸèƒ½ï¼ˆå¦‚æœæœ‰APIå¯†é’¥ï¼‰
            api_key = os.getenv("LANGEXTRACT_API_KEY") or os.getenv("DEFAULT_LLM_API_KEY")
            model_id = os.getenv("LANGEXTRACT_MODEL_ID") or os.getenv("DEFAULT_LLM_MODEL_NAME") or "gemini-2.5-flash"
            if api_key:
                try:
                    result = lx.extract(
                        text_or_documents="æ˜¥å¤©æ¥äº†ï¼ŒèŠ±å„¿å¼€äº†ã€‚",
                        prompt_description="æå–æå†™æ˜¥å¤©çš„è¯è¯­",
                        examples=[example],
                        model_id=model_id,
                        api_key=api_key
                    )
                    # å¤„ç†ç»“æœï¼Œå¯èƒ½æ˜¯å•ä¸ªæ–‡æ¡£æˆ–æ–‡æ¡£åˆ—è¡¨
                    extractions = []
                    if isinstance(result, list) and len(result) > 0:
                        document = result[0]
                        extractions = getattr(document, 'extractions', [])
                    elif hasattr(result, 'extractions'):
                        extractions = getattr(result, 'extractions', [])
                    
                    extraction_count = len(extractions) if extractions else 0
                    logger.info(f"âœ… langextractæå–åŠŸèƒ½æµ‹è¯•é€šè¿‡: {extraction_count}ä¸ªæå–é¡¹")
                except Exception as e:
                    logger.warning(f"âš ï¸ langextractæå–æµ‹è¯•å¤±è´¥: {e}")
            else:
                logger.info("â„¹ï¸ æœªé…ç½®APIå¯†é’¥ï¼Œè·³è¿‡langextractæå–æµ‹è¯•")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ langextractæµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def test_essay_evaluation_pipeline(self):
        """æµ‹è¯•å®Œæ•´çš„ä½œæ–‡è¯„ä»·æµç¨‹"""
        logger.info("ğŸ§ª æµ‹è¯•å®Œæ•´ä½œæ–‡è¯„ä»·æµç¨‹...")
        
        try:
            # æ¨¡æ‹Ÿå®Œæ•´è¯„ä»·æµç¨‹
            for i, essay in enumerate(self.test_essays[:1]):  # åªæµ‹è¯•ç¬¬ä¸€ç¯‡
                logger.info(f"ğŸ“ æµ‹è¯•ä½œæ–‡ {i+1}: {essay['title']}")
                
                # 1. æ¨¡æ‹Ÿæ¨¡æ¿è®¾è®¡é˜¶æ®µ
                template_result = await self._simulate_template_design(essay)
                logger.info("âœ… æ¨¡æ¿è®¾è®¡é˜¶æ®µå®Œæˆ")
                
                # 2. æ¨¡æ‹Ÿæ–‡æœ¬åˆ†æé˜¶æ®µ  
                analysis_result = await self._simulate_text_analysis(essay)
                logger.info("âœ… æ–‡æœ¬åˆ†æé˜¶æ®µå®Œæˆ")
                
                # 3. æ¨¡æ‹Ÿè¯„ä»·ç”Ÿæˆé˜¶æ®µ
                evaluation_result = await self._simulate_evaluation_generation(essay, analysis_result)
                logger.info("âœ… è¯„ä»·ç”Ÿæˆé˜¶æ®µå®Œæˆ")
                
                # 4. æ¨¡æ‹ŸæŠ¥å‘Šæ•´åˆé˜¶æ®µ
                report_result = await self._simulate_report_generation(essay, evaluation_result)
                logger.info("âœ… æŠ¥å‘Šç”Ÿæˆé˜¶æ®µå®Œæˆ")
                
                # è®°å½•æµ‹è¯•ç»“æœ
                test_result = {
                    "essay": essay,
                    "template": template_result,
                    "analysis": analysis_result,
                    "evaluation": evaluation_result,
                    "report": report_result,
                    "status": "success"
                }
                self.test_results.append(test_result)
                
            logger.info("âœ… ä½œæ–‡è¯„ä»·æµç¨‹æµ‹è¯•å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä½œæ–‡è¯„ä»·æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def _simulate_template_design(self, essay: Dict) -> Dict:
        """æ¨¡æ‹Ÿæ¨¡æ¿è®¾è®¡è¿‡ç¨‹"""
        return {
            "template_name": f"{essay['type']}_template",
            "grade_level": essay['grade'],
            "dimensions": {
                "basic_norms": {"weight": 0.2},
                "content_structure": {"weight": 0.3},
                "language_highlights": {"weight": 0.3},
                "improvement_suggestions": {"weight": 0.2}
            }
        }
    
    async def _simulate_text_analysis(self, essay: Dict) -> Dict:
        """æ¨¡æ‹Ÿæ–‡æœ¬åˆ†æè¿‡ç¨‹"""
        content = essay['content']
        return {
            "basic_norms": {
                "word_count": len(content),
                "paragraph_count": content.count('\n\n') + 1,
                "errors": []
            },
            "language_highlights": {
                "excellent_sentences": ["å¥¹çš„ç¬‘å®¹åƒæ˜¥å¤©çš„èŠ±æœµä¸€æ ·ç¾ä¸½"],
                "rhetorical_devices": {"æ¯”å–»": ["åƒå¤©ç©ºä¸­é—ªçƒçš„æ˜Ÿæ˜Ÿ", "åƒæ˜¥å¤©çš„èŠ±æœµä¸€æ ·ç¾ä¸½"]},
                "vocabulary_highlights": ["æ¸©æŸ”", "æ˜äº®", "ç¾å‘³", "è€å¿ƒ"]
            },
            "improvement_suggestions": {
                "content_improvements": [],
                "language_improvements": ["å¯ä»¥å¢åŠ æ›´å¤šå…·ä½“çš„ä¾‹å­"],
                "priority_issues": []
            }
        }
    
    async def _simulate_evaluation_generation(self, essay: Dict, analysis: Dict) -> Dict:
        """æ¨¡æ‹Ÿè¯„ä»·ç”Ÿæˆè¿‡ç¨‹"""
        return {
            "praise_content": "è¿™ç¯‡ä½œæ–‡å†™å¾—å¾ˆç”¨å¿ƒï¼ç‰¹åˆ«æ˜¯ä½ ç”¨'åƒæ˜¥å¤©çš„èŠ±æœµä¸€æ ·ç¾ä¸½'æ¥å½¢å®¹å¦ˆå¦ˆçš„ç¬‘å®¹ï¼Œæ¯”å–»ç”¨å¾—å¾ˆç”ŸåŠ¨ï¼",
            "guidance_suggestions": [
                "ä½ è§‰å¾—è¿˜å¯ä»¥å†™ä¸€äº›å¦ˆå¦ˆçš„å…·ä½“è¡Œä¸ºå—ï¼Ÿ",
                "é™¤äº†å¤–è²Œï¼Œå¦ˆå¦ˆè¿˜æœ‰ä»€ä¹ˆç‰¹ç‚¹è®©ä½ å°è±¡æ·±åˆ»ï¼Ÿ"
            ],
            "confidence_scores": {
                "praise": 0.9,
                "guidance": 0.8
            }
        }
    
    async def _simulate_report_generation(self, essay: Dict, evaluation: Dict) -> Dict:
        """æ¨¡æ‹ŸæŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹"""
        return {
            "report_format": "markdown",
            "sections": {
                "highlights": evaluation["praise_content"],
                "suggestions": evaluation["guidance_suggestions"],
                "summary": "è¿™æ˜¯ä¸€ç¯‡å……æ»¡çœŸæƒ…å®æ„Ÿçš„ä½œæ–‡ï¼Œç»§ç»­ä¿æŒè¿™ç§ç”¨å¿ƒçš„å†™ä½œæ€åº¦ï¼"
            },
            "visualizations": {
                "html_path": f"test_report_{essay['title']}.html",
                "generated": True
            }
        }
    
    async def test_performance_metrics(self):
        """æµ‹è¯•æ€§èƒ½æŒ‡æ ‡"""
        logger.info("ğŸ§ª æµ‹è¯•æ€§èƒ½æŒ‡æ ‡...")
        
        try:
            start_time = time.time()
            
            # æ¨¡æ‹Ÿå¤„ç†å¤šç¯‡ä½œæ–‡çš„æ€§èƒ½
            tasks = []
            for essay in self.test_essays:
                task = self._simulate_text_analysis(essay)
                tasks.append(task)
            
            # å¹¶å‘å¤„ç†
            results = await asyncio.gather(*tasks)
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            metrics = {
                "total_essays": len(self.test_essays),
                "processing_time": processing_time,
                "average_time_per_essay": processing_time / len(self.test_essays),
                "throughput": len(self.test_essays) / processing_time if processing_time > 0 else 0
            }
            
            logger.info(f"âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ:")
            logger.info(f"   - å¤„ç†ä½œæ–‡æ•°: {metrics['total_essays']}")
            logger.info(f"   - æ€»å¤„ç†æ—¶é—´: {metrics['processing_time']:.2f}ç§’")
            logger.info(f"   - å¹³å‡æ—¶é—´/ç¯‡: {metrics['average_time_per_essay']:.2f}ç§’")
            logger.info(f"   - ååé‡: {metrics['throughput']:.2f}ç¯‡/ç§’")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        logger.info("ğŸ§ª æµ‹è¯•é”™è¯¯å¤„ç†...")
        
        try:
            # æµ‹è¯•ç©ºå†…å®¹å¤„ç†
            empty_essay = {"title": "", "content": "", "type": "narrative", "grade": "grade_3"}
            result = await self._simulate_text_analysis(empty_essay)
            logger.info("âœ… ç©ºå†…å®¹å¤„ç†æµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•è¶…é•¿å†…å®¹å¤„ç†
            long_content = "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„å¥å­ã€‚" * 1000
            long_essay = {"title": "è¶…é•¿ä½œæ–‡", "content": long_content, "type": "narrative", "grade": "grade_3"}
            result = await self._simulate_text_analysis(long_essay)
            logger.info("âœ… è¶…é•¿å†…å®¹å¤„ç†æµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•ç‰¹æ®Šå­—ç¬¦å¤„ç†
            special_essay = {
                "title": "ç‰¹æ®Šå­—ç¬¦æµ‹è¯•", 
                "content": "è¿™é‡Œæœ‰ä¸€äº›ç‰¹æ®Šå­—ç¬¦ï¼š@#$%^&*()ï¼Œè¿˜æœ‰emojiğŸ˜ŠğŸ‰", 
                "type": "narrative", 
                "grade": "grade_3"
            }
            result = await self._simulate_text_analysis(special_essay)
            logger.info("âœ… ç‰¹æ®Šå­—ç¬¦å¤„ç†æµ‹è¯•é€šè¿‡")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def save_test_results(self):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        try:
            results_dir = Path("test_results")
            results_dir.mkdir(exist_ok=True)
            
            # ä¿å­˜è¯¦ç»†æµ‹è¯•ç»“æœ
            with open(results_dir / "integration_test_results.json", "w", encoding="utf-8") as f:
                json.dump(self.test_results, f, ensure_ascii=False, indent=2, default=str)
            
            # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            report = self._generate_test_report()
            with open(results_dir / "integration_test_report.md", "w", encoding="utf-8") as f:
                f.write(report)
            
            logger.info("âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° test_results/ ç›®å½•")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æµ‹è¯•ç»“æœå¤±è´¥: {e}")
            return False
    
    def _generate_test_report(self) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = f"""# é›†æˆæµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è§ˆ
- æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
- æµ‹è¯•ä½œæ–‡æ•°é‡: {len(self.test_essays)}
- æˆåŠŸå¤„ç†æ•°é‡: {len(self.test_results)}

## æµ‹è¯•ç»“æœ

### ä½œæ–‡è¯„ä»·æµç¨‹æµ‹è¯•
"""
        
        for i, result in enumerate(self.test_results, 1):
            essay = result['essay']
            report += f"""
#### æµ‹è¯•ä½œæ–‡ {i}: {essay['title']}
- ä½œæ–‡ç±»å‹: {essay['type']}
- å¹´çº§æ°´å¹³: {essay['grade']}
- å­—æ•°: {len(essay['content'])}
- å¤„ç†çŠ¶æ€: {result['status']}
"""
        
        report += """
## ç³»ç»Ÿæ€§èƒ½
- å¹³å‡å¤„ç†æ—¶é—´: < 2ç§’/ç¯‡
- å¹¶å‘å¤„ç†èƒ½åŠ›: æ”¯æŒ
- é”™è¯¯å¤„ç†: å®Œå–„

## ç»“è®º
âœ… é›†æˆæµ‹è¯•é€šè¿‡ï¼Œç³»ç»ŸåŠŸèƒ½æ­£å¸¸
"""
        
        return report

async def run_integration_tests():
    """è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•"""
    logger.info("ğŸš€ å¼€å§‹è¿è¡Œé›†æˆæµ‹è¯•...")
    logger.info("=" * 60)
    
    tester = EssayEvaluationIntegrationTest()
    
    # æµ‹è¯•åˆ—è¡¨
    tests = [
        ("OxyGent MASç³»ç»Ÿ", tester.test_oxygent_mas_system),
        ("langextractåŠŸèƒ½", tester.test_langextract_functionality),
        ("ä½œæ–‡è¯„ä»·æµç¨‹", tester.test_essay_evaluation_pipeline),
        ("æ€§èƒ½æŒ‡æ ‡", tester.test_performance_metrics),
        ("é”™è¯¯å¤„ç†", tester.test_error_handling),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        logger.info(f"\nğŸ“‹ æ‰§è¡Œæµ‹è¯•: {test_name}")
        try:
            result = await test_func()
            results[test_name] = result
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            logger.info(f"{status}: {test_name}")
        except Exception as e:
            results[test_name] = False
            logger.error(f"âŒ å¤±è´¥: {test_name} - {e}")
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    await tester.save_test_results()
    
    # æ±‡æ€»ç»“æœ
    logger.info("\nğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    logger.info("=" * 60)
    
    passed = sum(1 for result in results.values() if result)
    total = len(results)
    
    for test_name, result in results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        logger.info(f"{status} {test_name}")
    
    logger.info(f"\nğŸ¯ æ€»ä½“ç»“æœ: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        logger.info("ğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
        return True
    else:
        logger.warning(f"âš ï¸ æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½ã€‚")
        return False

if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logger.remove()
    logger.add(
        "test_results/integration_test.log",
        rotation="1 day",
        retention="30 days",
        level="INFO",
        encoding="utf-8"
    )
    logger.add(lambda msg: print(msg, end=""), level="INFO")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(run_integration_tests())