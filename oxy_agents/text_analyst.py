"""
æ–‡æœ¬åˆ†æå¸ˆæ™ºèƒ½ä½“

åŸºäºOxyGent ChatAgent + langextractçš„å®ç°
"""

import asyncio
import textwrap
from typing import Dict, List, Any, Optional

import langextract as lx
from loguru import logger
from oxygent import oxy, OxyRequest
from oxygent.utils.env_utils import get_env_var

from prompts.analyst_prompts import SYSTEM_PROMPT, LANGEXTRACT_ANALYSIS_PROMPT
from langextract_schemas.schema_templates import SchemaTemplateManager, ExtractionTemplate, EssayType, GradeLevel


def create_text_analyst_agent() -> oxy.ChatAgent:
    """
    åˆ›å»ºæ–‡æœ¬åˆ†æå¸ˆæ™ºèƒ½ä½“
    
    åŸºäºOxyGent ChatAgent + langextractçš„é›†æˆå®ç°
    """
    return oxy.ChatAgent(  # type: ignore
        name="text_analyst",
        desc="ä¸¥è°¨çš„æ–‡æœ¬åˆ†æä¸“å®¶ï¼Œä½¿ç”¨langextractè¿›è¡Œæ·±åº¦ç»“æ„åŒ–åˆ†æ",
        prompt=SYSTEM_PROMPT
    )


def process_analysis_input(oxy_request: OxyRequest) -> OxyRequest:
    """
    å¤„ç†æ–‡æœ¬åˆ†æè¾“å…¥è¯·æ±‚
    
    é›†æˆlangextractåŠŸèƒ½åˆ°OxyGentå·¥ä½œæµä¸­
    """
    try:
        # è·å–ç”¨æˆ·æŸ¥è¯¢
        user_query = oxy_request.get_query(master_level=True)
        current_query = oxy_request.get_query()
        
        # å¦‚æœæŸ¥è¯¢ä¸­åŒ…å«ä½œæ–‡åˆ†æä»»åŠ¡ï¼Œè¿›è¡Œé¢„å¤„ç†
        if "åˆ†æä½œæ–‡" in current_query or "æ–‡æœ¬åˆ†æ" in current_query:
            helper = TextAnalystHelper()
            
            # æå–ä½œæ–‡å†…å®¹ï¼ˆå‡è®¾åœ¨argumentsä¸­ï¼‰
            essay_content = oxy_request.arguments.get("essay_content", "")
            essay_type = oxy_request.arguments.get("essay_type", "narrative")
            grade_level = oxy_request.arguments.get("grade_level", "grade_3")
            
            if essay_content:
                # å¼‚æ­¥æ‰§è¡Œåˆ†æï¼ˆåœ¨å®é™…ä½¿ç”¨ä¸­ä¼šè¢«OxyGentæ¡†æ¶å¤„ç†ï¼‰
                analysis_prompt = helper.prepare_analysis_prompt(
                    essay_content, essay_type, grade_level
                )
                oxy_request.set_query(analysis_prompt)
        
        return oxy_request
        
    except Exception as e:
        logger.error(f"å¤„ç†åˆ†æè¾“å…¥å¤±è´¥: {e}")
        return oxy_request


class TextAnalystHelper:
    """
    æ–‡æœ¬åˆ†æå¸ˆè¾…åŠ©ç±»
    
    æä¾›langextracté›†æˆå’Œæ•°æ®å¤„ç†åŠŸèƒ½
    """
    
    def __init__(self):
        self.schema_manager = SchemaTemplateManager()
        self.name = "text_analyst_helper"
        self.description = "æ–‡æœ¬åˆ†æè¾…åŠ©å·¥å…·ï¼Œå¤„ç†langextracté›†æˆ"
    
    def prepare_analysis_prompt(
        self,
        essay_content: str,
        essay_type: str,
        grade_level: str,
        schema_template: Optional[ExtractionTemplate] = None
    ) -> str:
        """
        å‡†å¤‡åˆ†ææç¤ºè¯
        
        Args:
            essay_content: ä½œæ–‡å†…å®¹
            essay_type: ä½œæ–‡ç±»å‹
            grade_level: å¹´çº§æ°´å¹³
            schema_template: æŒ‡å®šçš„è¯„ä»·æ¨¡æ¿
            
        Returns:
            æ ¼å¼åŒ–çš„åˆ†ææç¤ºè¯
        """
        try:
            # è·å–åˆé€‚çš„æå–æ¨¡æ¿
            if not schema_template:
                # è½¬æ¢å­—ç¬¦ä¸²å‚æ•°ä¸ºæšä¸¾ç±»å‹
                essay_type_enum = EssayType(essay_type) if essay_type in [e.value for e in EssayType] else EssayType.NARRATIVE
                grade_level_enum = GradeLevel(grade_level) if grade_level in [g.value for g in GradeLevel] else GradeLevel.GRADE_3
                
                schema_template = self.schema_manager.get_template_by_type(
                    essay_type_enum, grade_level_enum
                )
            
            # æ ¼å¼åŒ–æç¤ºè¯
            formatted_prompt = LANGEXTRACT_ANALYSIS_PROMPT.format(
                essay_content=essay_content,
                schema_description=schema_template.prompt
            )
            
            return formatted_prompt
            
        except Exception as e:
            logger.error(f"å‡†å¤‡åˆ†ææç¤ºè¯å¤±è´¥: {e}")
            return f"è¯·åˆ†æä»¥ä¸‹ä½œæ–‡ï¼š\n{essay_content}"
    
    async def analyze_essay(
        self,
        essay_content: str,
        essay_type: str,
        grade_level: str,
        schema_template: Optional[ExtractionTemplate] = None
    ) -> Dict[str, Any]:
        """
        åˆ†æä½œæ–‡å†…å®¹
        
        Args:
            essay_content: ä½œæ–‡å†…å®¹
            essay_type: ä½œæ–‡ç±»å‹
            grade_level: å¹´çº§æ°´å¹³
            schema_template: æŒ‡å®šçš„è¯„ä»·æ¨¡æ¿
            
        Returns:
            ç»“æ„åŒ–åˆ†æç»“æœ
        """
        logger.info(f"å¼€å§‹åˆ†æä½œæ–‡ï¼Œç±»å‹: {essay_type}, å¹´çº§: {grade_level}")
        
        try:
            # è·å–åˆé€‚çš„æå–æ¨¡æ¿
            if not schema_template:
                # è½¬æ¢å­—ç¬¦ä¸²å‚æ•°ä¸ºæšä¸¾ç±»å‹
                essay_type_enum = EssayType(essay_type) if essay_type in [e.value for e in EssayType] else EssayType.NARRATIVE
                grade_level_enum = GradeLevel(grade_level) if grade_level in [g.value for g in GradeLevel] else GradeLevel.GRADE_3
                
                schema_template = self.schema_manager.get_template_by_type(
                    essay_type_enum, grade_level_enum
                )
            
            # ä½¿ç”¨langextractè¿›è¡Œç»“æ„åŒ–æå–
            extraction_result = await self._perform_langextract_analysis(
                essay_content, schema_template
            )
            
            # æ•´ç†åˆ†æç»“æœ
            analysis_result = self._organize_analysis_result(
                extraction_result, essay_content
            )
            
            logger.info("ä½œæ–‡åˆ†æå®Œæˆ")
            return analysis_result
            
        except Exception as e:
            logger.error(f"ä½œæ–‡åˆ†æå¤±è´¥: {e}")
            return self._get_fallback_analysis(essay_content)
    
    async def _perform_langextract_analysis(
        self,
        text: str,
        template: ExtractionTemplate
    ) -> lx.data.AnnotatedDocument:
        """
        æ‰§è¡Œlangextractåˆ†æ
        """
        try:
            # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
            api_key = get_env_var("LANGEXTRACT_API_KEY") or get_env_var("DEFAULT_LLM_API_KEY")
            model_id = get_env_var("LANGEXTRACT_MODEL_ID") or get_env_var("DEFAULT_LLM_MODEL_NAME") or "gemini-2.5-flash"
            base_url = get_env_var("LANGEXTRACT_BASE_URL") or get_env_var("DEFAULT_LLM_BASE_URL")
            
            # æ„å»ºåŸºç¡€å‚æ•°
            extract_kwargs = {
                "text_or_documents": text,
                "prompt_description": template.prompt,
                "examples": template.examples,
                "model_id": model_id,
                "extraction_passes": 2,  # å¤šæ¬¡æå–æé«˜å‡†ç¡®æ€§
                "max_workers": 4,
                "max_char_buffer": 2000  # é€‚åˆä½œæ–‡é•¿åº¦çš„ç¼“å†²åŒº
            }
            
            # å¦‚æœæœ‰APIå¯†é’¥ï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
            if api_key:
                extract_kwargs["api_key"] = api_key
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç¬¬ä¸‰æ–¹OpenAIå…¼å®¹API
            if base_url and base_url != "https://api.openai.com/v1":
                logger.info(f"ä½¿ç”¨ç¬¬ä¸‰æ–¹OpenAIå…¼å®¹API: {base_url}")
                # ä½¿ç”¨OpenAIè¯­è¨€æ¨¡å‹ç±»å‹ï¼Œå¹¶é€šè¿‡language_model_paramsä¼ é€’base_url
                extract_kwargs["language_model_type"] = lx.inference.OpenAILanguageModel
                extract_kwargs["language_model_params"] = {
                    "base_url": base_url
                }
                # OpenAIæ¨¡å‹éœ€è¦ç‰¹å®šå‚æ•°
                extract_kwargs["fence_output"] = True
                extract_kwargs["use_schema_constraints"] = False
            else:
                logger.info("ä½¿ç”¨é»˜è®¤langextracté…ç½®")
            
            result = lx.extract(**extract_kwargs)
            
            # å¤„ç†ç»“æœï¼Œå¯èƒ½æ˜¯å•ä¸ªæ–‡æ¡£æˆ–æ–‡æ¡£åˆ—è¡¨
            if isinstance(result, list) and len(result) > 0:
                document = result[0]  # type: ignore
            elif hasattr(result, '__iter__') and not isinstance(result, str):
                # å¦‚æœæ˜¯å¯è¿­ä»£å¯¹è±¡ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨å¹¶å–ç¬¬ä¸€ä¸ª
                result_list = list(result)  # type: ignore
                document = result_list[0] if result_list else result  # type: ignore
            else:
                document = result  # type: ignore
            
            extractions = getattr(document, 'extractions', [])
            logger.debug(f"langextractæå–å®Œæˆï¼Œè·å¾— {len(extractions) if extractions else 0} ä¸ªæå–é¡¹")
            return document  # type: ignore
            
        except Exception as e:
            logger.error(f"langextractåˆ†æå¤±è´¥: {e}")
            raise
    
    def _organize_analysis_result(
        self,
        extraction_result: lx.data.AnnotatedDocument,
        original_text: str
    ) -> Dict[str, Any]:
        """
        æ•´ç†åˆ†æç»“æœä¸ºç»“æ„åŒ–æ•°æ®
        """
        organized_result = {
            "basic_norms": {
                "word_count": len(original_text),
                "paragraph_count": original_text.count('\n\n') + 1,
                "errors": [],
                "format_issues": []
            },
            "content_structure": {
                "main_ideas": [],
                "key_elements": {},
                "structure_analysis": []
            },
            "language_highlights": {
                "excellent_sentences": [],
                "rhetorical_devices": {},
                "vocabulary_highlights": [],
                "expression_techniques": []
            },
            "improvement_suggestions": {
                "content_improvements": [],
                "language_improvements": [],
                "structure_improvements": [],
                "priority_issues": []
            },
            "metadata": {
                "total_extractions": len(getattr(extraction_result, 'extractions', [])),
                "analysis_timestamp": (
                    getattr(extraction_result, 'created_at', None).isoformat()  # type: ignore
                    if hasattr(extraction_result, 'created_at') and 
                       getattr(extraction_result, 'created_at', None) is not None 
                    else None
                ),
                "template_used": extraction_result.__class__.__name__
            }
        }
        
        # æŒ‰ç±»åˆ«æ•´ç†æå–ç»“æœ
        extractions = getattr(extraction_result, 'extractions', [])
        if extractions:
            for extraction in extractions:  # type: ignore
                category = extraction.extraction_class.lower()  # type: ignore
                
                if category in ["é”™åˆ«å­—", "æ ‡ç‚¹é”™è¯¯", "è¯­æ³•é—®é¢˜"]:
                    organized_result["basic_norms"]["errors"].append({
                        "type": category,
                        "text": extraction.extraction_text,  # type: ignore
                        "position": self._get_position_info(extraction),
                        "attributes": extraction.attributes or {}  # type: ignore
                    })
                
                elif category in ["ä¸­å¿ƒæ€æƒ³", "ä¸»é¢˜", "main_idea"]:
                    organized_result["content_structure"]["main_ideas"].append({
                        "text": extraction.extraction_text,  # type: ignore
                        "position": self._get_position_info(extraction),
                        "attributes": extraction.attributes or {}  # type: ignore
                    })
                
                elif category in ["ä¿®è¾æ‰‹æ³•", "rhetorical_device"]:
                    device_type = extraction.attributes.get("rhetorical_type", "å…¶ä»–") if extraction.attributes else "å…¶ä»–"  # type: ignore
                    if device_type not in organized_result["language_highlights"]["rhetorical_devices"]:
                        organized_result["language_highlights"]["rhetorical_devices"][device_type] = []
                    
                    organized_result["language_highlights"]["rhetorical_devices"][device_type].append({
                        "text": extraction.extraction_text,  # type: ignore
                        "position": self._get_position_info(extraction),
                        "effect": extraction.attributes.get("effect", "") if extraction.attributes else ""  # type: ignore
                    })
                
                elif category in ["äº®ç‚¹å¥å­", "highlight_sentence", "ä¼˜ç¾å¥å­"]:
                    organized_result["language_highlights"]["excellent_sentences"].append({
                        "text": extraction.extraction_text,  # type: ignore
                        "position": self._get_position_info(extraction),
                        "highlight_type": extraction.attributes.get("highlight_type", "") if extraction.attributes else "",  # type: ignore
                        "reason": extraction.attributes.get("reason", "") if extraction.attributes else ""  # type: ignore
                    })
                
                elif category in ["æ”¹è¿›å»ºè®®", "improvement", "é—®é¢˜"]:
                    organized_result["improvement_suggestions"]["content_improvements"].append({
                        "issue": extraction.extraction_text,  # type: ignore
                        "position": self._get_position_info(extraction),
                        "suggestion": extraction.attributes.get("suggestion", "") if extraction.attributes else "",  # type: ignore
                        "priority": extraction.attributes.get("priority", "medium") if extraction.attributes else "medium"  # type: ignore
                    })
        
        return organized_result
    
    def _get_position_info(self, extraction: lx.data.Extraction) -> Dict[str, Any]:
        """è·å–æå–é¡¹çš„ä½ç½®ä¿¡æ¯"""
        if hasattr(extraction, 'char_interval') and extraction.char_interval:
            return {
                "start": extraction.char_interval.start_pos,
                "end": extraction.char_interval.end_pos,
                "has_position": True
            }
        return {"has_position": False}
    
    def _get_fallback_analysis(self, text: str) -> Dict[str, Any]:
        """è·å–å¤‡ç”¨åˆ†æç»“æœï¼ˆå½“langextractå¤±è´¥æ—¶ï¼‰"""
        logger.warning("ä½¿ç”¨å¤‡ç”¨åˆ†ææ–¹æ³•")
        
        return {
            "basic_norms": {
                "word_count": len(text),
                "paragraph_count": text.count('\n\n') + 1,
                "errors": [],
                "format_issues": []
            },
            "content_structure": {
                "main_ideas": [{"text": "éœ€è¦è¿›ä¸€æ­¥åˆ†æ", "position": {"has_position": False}}],
                "key_elements": {},
                "structure_analysis": []
            },
            "language_highlights": {
                "excellent_sentences": [],
                "rhetorical_devices": {},
                "vocabulary_highlights": [],
                "expression_techniques": []
            },
            "improvement_suggestions": {
                "content_improvements": [],
                "language_improvements": [],
                "structure_improvements": [],
                "priority_issues": []
            },
            "metadata": {
                "total_extractions": 0,
                "analysis_timestamp": None,
                "template_used": "fallback",
                "note": "ä½¿ç”¨å¤‡ç”¨åˆ†ææ–¹æ³•ï¼Œå»ºè®®æ£€æŸ¥langextracté…ç½®"
            }
        }
    
    async def generate_visualization(
        self,
        analysis_result: Dict[str, Any],
        original_text: str,
        output_path: str = "essay_analysis_visualization.html"
    ) -> str:
        """
        ç”Ÿæˆlangextractå¯è§†åŒ–
        
        Args:
            analysis_result: åˆ†æç»“æœ
            original_text: åŸæ–‡
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            ç”Ÿæˆçš„HTMLæ–‡ä»¶è·¯å¾„
        """
        try:
            # é‡æ„ä¸ºlangextractæ ¼å¼çš„æ•°æ®
            extractions = []
            
            # æ·»åŠ äº®ç‚¹å¥å­
            for sentence in analysis_result["language_highlights"]["excellent_sentences"]:
                extractions.append(lx.data.Extraction(
                    extraction_class="äº®ç‚¹å¥å­",
                    extraction_text=sentence["text"],
                    attributes={"type": sentence.get("highlight_type", "")}
                ))
            
            # æ·»åŠ ä¿®è¾æ‰‹æ³•
            for device_type, devices in analysis_result["language_highlights"]["rhetorical_devices"].items():
                for device in devices:
                    extractions.append(lx.data.Extraction(
                        extraction_class="ä¿®è¾æ‰‹æ³•",
                        extraction_text=device["text"],
                        attributes={"rhetorical_type": device_type, "effect": device.get("effect", "")}
                    ))
            
            # åˆ›å»ºAnnotatedDocument
            document = lx.data.AnnotatedDocument(
                text=original_text,
                extractions=extractions
            )
            
            # ä¿å­˜ä¸ºJSONLæ ¼å¼
            jsonl_path = output_path.replace('.html', '.jsonl')
            lx.io.save_annotated_documents(iter([document]), output_name=jsonl_path.split('.')[0], output_dir=".")
            
            # ç”Ÿæˆå¯è§†åŒ–
            html_content = lx.visualize(jsonl_path)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            logger.info(f"å¯è§†åŒ–æ–‡ä»¶å·²ç”Ÿæˆ: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¯è§†åŒ–å¤±è´¥: {e}")
            return ""
    
    def get_analysis_summary(self, analysis_result: Dict[str, Any]) -> str:
        """
        è·å–åˆ†ææ‘˜è¦
        """
        highlights_count = len(analysis_result["language_highlights"]["excellent_sentences"])
        rhetorical_count = sum(len(devices) for devices in analysis_result["language_highlights"]["rhetorical_devices"].values())
        issues_count = len(analysis_result["improvement_suggestions"]["content_improvements"])
        
        summary = f"""
ğŸ“Š **åˆ†ææ‘˜è¦**
- å­—æ•°: {analysis_result['basic_norms']['word_count']}å­—
- æ®µè½æ•°: {analysis_result['basic_norms']['paragraph_count']}æ®µ
- å‘ç°äº®ç‚¹: {highlights_count}å¤„
- ä¿®è¾æ‰‹æ³•: {rhetorical_count}å¤„
- æ”¹è¿›å»ºè®®: {issues_count}æ¡
        """
        
        return summary.strip()


# =================== æ¨¡å—å¯¼å‡ºå‡½æ•° ===================

def get_text_analyst_config() -> dict:
    """
    è·å–æ–‡æœ¬åˆ†æå¸ˆé…ç½®
    
    è¿”å›OxyGent ChatAgentæ‰€éœ€çš„é…ç½®å‚æ•°
    """
    return {
        "name": "text_analyst",
        "desc": "ä¸¥è°¨çš„æ–‡æœ¬åˆ†æä¸“å®¶ï¼Œä½¿ç”¨langextractè¿›è¡Œæ·±åº¦ç»“æ„åŒ–åˆ†æ",
        "llm_model": "default_llm",
        "prompt": SYSTEM_PROMPT,
        "func_process_input": process_analysis_input
    }


# =================== ç¤ºä¾‹ç”¨æ³• ===================

if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šåˆ›å»ºå’Œæµ‹è¯•æ–‡æœ¬åˆ†æå¸ˆ
    helper = TextAnalystHelper()
    
    # æµ‹è¯•ä½œæ–‡
    test_essay = """
    æ˜¥å¤©çš„å…¬å›­é‡Œï¼Œæ¨±èŠ±ç››å¼€ï¼Œç²‰çº¢è‰²çš„èŠ±ç“£å¦‚é›ªèŠ±èˆ¬é£˜æ´’ã€‚
    å¾®é£è½»æ‹‚ï¼Œå¸¦æ¥é˜µé˜µèŠ±é¦™ï¼Œæ²äººå¿ƒè„¾ã€‚
    å°é¸Ÿåœ¨æå¤´æ¬¢å¿«åœ°æ­Œå”±ï¼Œæ¸…è„†æ‚¦è€³ã€‚
    å­©å­ä»¬åœ¨è‰åœ°ä¸Šå¬‰æˆï¼Œæ¬¢å£°ç¬‘è¯­å›è¡åœ¨ç©ºä¸­ã€‚
    """
    
    # æ¸…ç†æµ‹è¯•æ–‡æœ¬
    test_essay = test_essay.strip().replace("\n    ", "\n")
    
    # ç”Ÿæˆåˆ†ææç¤ºè¯
    prompt = helper.prepare_analysis_prompt(
        essay_content=test_essay,
        essay_type="descriptive",
        grade_level="grade_4"
    )
    
    print("ğŸ“„ ç”Ÿæˆçš„åˆ†ææç¤ºè¯ï¼š")
    print(prompt)
    print("\n" + "="*50 + "\n")
    
    print("ğŸ† æ–‡æœ¬åˆ†æå¸ˆé…ç½®ï¼š")
    config = get_text_analyst_config()
    for key, value in config.items():
        if key != "prompt":  # promptå¤ªé•¿ï¼Œä¸å®Œæ•´æ˜¾ç¤º
            print(f"  {key}: {value}")
        else:
            print(f"  {key}: [ç³»ç»Ÿæç¤ºè¯]")